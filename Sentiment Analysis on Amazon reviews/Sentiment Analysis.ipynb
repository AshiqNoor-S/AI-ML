{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e3c5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da448008",
   "metadata": {},
   "source": [
    "For this sentiment analysis ,I have used the Amazon Customer Reviews Dataset. This dataset contains product reviews from Amazon and is widely used for sentiment analysis tasks. It is publicly available on the Amazon Customer Reviews Dataset page on the Amazon Web Services (AWS) website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc83b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0   pos  Stuning even for the non-gamer: This sound tra...\n",
       "1   pos  The best soundtrack ever to anything.: I'm rea...\n",
       "2   pos  Amazing!: This soundtrack is my favorite music...\n",
       "3   pos  Excellent Soundtrack: I truly like this soundt...\n",
       "4   pos  Remember, Pull Your Jaw Off The Floor After He..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('amazonreviews.tsv', sep='\\t', dtype=str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72591b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the selected dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d3843",
   "metadata": {},
   "source": [
    "# Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c98f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing null values\n",
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#removing empty strings \n",
    "blanks = [] \n",
    "for i,lb,rv in df.itertuples():  \n",
    "    if type(rv)==str:            \n",
    "        if rv.isspace():         \n",
    "            blanks.append(i)     \n",
    "        \n",
    "df.drop(blanks, inplace=True)\n",
    "\n",
    "#split data-set to train and test\n",
    "X=df['review']\n",
    "y=df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9be2d3",
   "metadata": {},
   "source": [
    "# Model 1 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd2b9903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a41f6_row0_col0, #T_a41f6_row1_col1, #T_a41f6_row1_col2, #T_a41f6_row2_col2, #T_a41f6_row2_col3, #T_a41f6_row3_col2, #T_a41f6_row4_col2 {\n",
       "  background-color: #f0f0f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a41f6_row0_col1, #T_a41f6_row0_col2, #T_a41f6_row1_col0, #T_a41f6_row3_col3, #T_a41f6_row4_col3 {\n",
       "  background-color: #0000ff;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a41f6_row0_col3 {\n",
       "  background-color: #7979f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a41f6_row1_col3 {\n",
       "  background-color: #7878f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a41f6_row2_col0, #T_a41f6_row3_col0, #T_a41f6_row4_col0 {\n",
       "  background-color: #a0a0f7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a41f6_row2_col1, #T_a41f6_row3_col1, #T_a41f6_row4_col1 {\n",
       "  background-color: #9090f8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a41f6_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >precision</th>\n",
       "      <th class=\"col_heading level0 col1\" >recall</th>\n",
       "      <th class=\"col_heading level0 col2\" >f1-score</th>\n",
       "      <th class=\"col_heading level0 col3\" >support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a41f6_level0_row0\" class=\"row_heading level0 row0\" >neg</th>\n",
       "      <td id=\"T_a41f6_row0_col0\" class=\"data row0 col0\" >0.840000</td>\n",
       "      <td id=\"T_a41f6_row0_col1\" class=\"data row0 col1\" >0.880000</td>\n",
       "      <td id=\"T_a41f6_row0_col2\" class=\"data row0 col2\" >0.860000</td>\n",
       "      <td id=\"T_a41f6_row0_col3\" class=\"data row0 col3\" >1649.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a41f6_level0_row1\" class=\"row_heading level0 row1\" >pos</th>\n",
       "      <td id=\"T_a41f6_row1_col0\" class=\"data row1 col0\" >0.870000</td>\n",
       "      <td id=\"T_a41f6_row1_col1\" class=\"data row1 col1\" >0.830000</td>\n",
       "      <td id=\"T_a41f6_row1_col2\" class=\"data row1 col2\" >0.850000</td>\n",
       "      <td id=\"T_a41f6_row1_col3\" class=\"data row1 col3\" >1651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a41f6_level0_row2\" class=\"row_heading level0 row2\" >accuracy</th>\n",
       "      <td id=\"T_a41f6_row2_col0\" class=\"data row2 col0\" >0.850000</td>\n",
       "      <td id=\"T_a41f6_row2_col1\" class=\"data row2 col1\" >0.850000</td>\n",
       "      <td id=\"T_a41f6_row2_col2\" class=\"data row2 col2\" >0.850000</td>\n",
       "      <td id=\"T_a41f6_row2_col3\" class=\"data row2 col3\" >0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a41f6_level0_row3\" class=\"row_heading level0 row3\" >macro avg</th>\n",
       "      <td id=\"T_a41f6_row3_col0\" class=\"data row3 col0\" >0.850000</td>\n",
       "      <td id=\"T_a41f6_row3_col1\" class=\"data row3 col1\" >0.850000</td>\n",
       "      <td id=\"T_a41f6_row3_col2\" class=\"data row3 col2\" >0.850000</td>\n",
       "      <td id=\"T_a41f6_row3_col3\" class=\"data row3 col3\" >3300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a41f6_level0_row4\" class=\"row_heading level0 row4\" >weighted avg</th>\n",
       "      <td id=\"T_a41f6_row4_col0\" class=\"data row4 col0\" >0.850000</td>\n",
       "      <td id=\"T_a41f6_row4_col1\" class=\"data row4 col1\" >0.850000</td>\n",
       "      <td id=\"T_a41f6_row4_col2\" class=\"data row4 col2\" >0.850000</td>\n",
       "      <td id=\"T_a41f6_row4_col3\" class=\"data row4 col3\" >3300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1983cddf340>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model=Pipeline([('tfidf', TfidfVectorizer(lowercase=False)),( 'clf',LogisticRegression(solver='lbfgs'))])\n",
    "lr_model.fit(X_train,y_train)\n",
    "\n",
    "predictions= lr_model.predict(X_test)\n",
    "report = classification_report(y_test,predictions, output_dict=True)\n",
    "\n",
    "df_report = pd.DataFrame(report).transpose().round(2)\n",
    "\n",
    "cm = sns.light_palette(\"blue\", as_cmap=True)\n",
    "df_report.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf2f99",
   "metadata": {},
   "source": [
    "# Model 2 : Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "400acfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_26a76_row0_col0, #T_26a76_row0_col2, #T_26a76_row1_col1, #T_26a76_row1_col2, #T_26a76_row2_col2, #T_26a76_row2_col3, #T_26a76_row3_col2, #T_26a76_row4_col2 {\n",
       "  background-color: #f0f0f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_26a76_row0_col1, #T_26a76_row1_col0, #T_26a76_row3_col3, #T_26a76_row4_col3 {\n",
       "  background-color: #0000ff;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_26a76_row0_col3 {\n",
       "  background-color: #7979f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_26a76_row1_col3, #T_26a76_row2_col1, #T_26a76_row3_col1, #T_26a76_row4_col1 {\n",
       "  background-color: #7878f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_26a76_row2_col0, #T_26a76_row3_col0, #T_26a76_row4_col0 {\n",
       "  background-color: #a0a0f7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_26a76_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >precision</th>\n",
       "      <th class=\"col_heading level0 col1\" >recall</th>\n",
       "      <th class=\"col_heading level0 col2\" >f1-score</th>\n",
       "      <th class=\"col_heading level0 col3\" >support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_26a76_level0_row0\" class=\"row_heading level0 row0\" >neg</th>\n",
       "      <td id=\"T_26a76_row0_col0\" class=\"data row0 col0\" >0.860000</td>\n",
       "      <td id=\"T_26a76_row0_col1\" class=\"data row0 col1\" >0.890000</td>\n",
       "      <td id=\"T_26a76_row0_col2\" class=\"data row0 col2\" >0.870000</td>\n",
       "      <td id=\"T_26a76_row0_col3\" class=\"data row0 col3\" >1649.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26a76_level0_row1\" class=\"row_heading level0 row1\" >pos</th>\n",
       "      <td id=\"T_26a76_row1_col0\" class=\"data row1 col0\" >0.890000</td>\n",
       "      <td id=\"T_26a76_row1_col1\" class=\"data row1 col1\" >0.850000</td>\n",
       "      <td id=\"T_26a76_row1_col2\" class=\"data row1 col2\" >0.870000</td>\n",
       "      <td id=\"T_26a76_row1_col3\" class=\"data row1 col3\" >1651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26a76_level0_row2\" class=\"row_heading level0 row2\" >accuracy</th>\n",
       "      <td id=\"T_26a76_row2_col0\" class=\"data row2 col0\" >0.870000</td>\n",
       "      <td id=\"T_26a76_row2_col1\" class=\"data row2 col1\" >0.870000</td>\n",
       "      <td id=\"T_26a76_row2_col2\" class=\"data row2 col2\" >0.870000</td>\n",
       "      <td id=\"T_26a76_row2_col3\" class=\"data row2 col3\" >0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26a76_level0_row3\" class=\"row_heading level0 row3\" >macro avg</th>\n",
       "      <td id=\"T_26a76_row3_col0\" class=\"data row3 col0\" >0.870000</td>\n",
       "      <td id=\"T_26a76_row3_col1\" class=\"data row3 col1\" >0.870000</td>\n",
       "      <td id=\"T_26a76_row3_col2\" class=\"data row3 col2\" >0.870000</td>\n",
       "      <td id=\"T_26a76_row3_col3\" class=\"data row3 col3\" >3300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26a76_level0_row4\" class=\"row_heading level0 row4\" >weighted avg</th>\n",
       "      <td id=\"T_26a76_row4_col0\" class=\"data row4 col0\" >0.870000</td>\n",
       "      <td id=\"T_26a76_row4_col1\" class=\"data row4 col1\" >0.870000</td>\n",
       "      <td id=\"T_26a76_row4_col2\" class=\"data row4 col2\" >0.870000</td>\n",
       "      <td id=\"T_26a76_row4_col3\" class=\"data row4 col3\" >3300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1983aa74370>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model\n",
    "my_model=Pipeline([('tfidf', TfidfVectorizer()),('classifier',LinearSVC())])\n",
    "my_model.fit(X_train,y_train)\n",
    "\n",
    "predictions= my_model.predict(X_test)\n",
    "report = classification_report(y_test,predictions, output_dict=True)\n",
    "\n",
    "df_report = pd.DataFrame(report).transpose().round(2)\n",
    "\n",
    "cm = sns.light_palette(\"blue\", as_cmap=True)\n",
    "df_report.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e667d25",
   "metadata": {},
   "source": [
    "# Model 3 : Vader's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "722ec95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93b63f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Vader sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row['review']\n",
    "    true_sentiment = row['label']\n",
    "    \n",
    "    # Get the predicted sentiment using the Vader model\n",
    "    scores = vader.polarity_scores(text)\n",
    "    predicted_sentiment = 'pos' if scores['compound'] >= 0 else 'neg'\n",
    "    \n",
    "    # Append the true and predicted labels to the respective lists\n",
    "    true_labels.append(true_sentiment)\n",
    "    predicted_labels.append(predicted_sentiment)\n",
    "\n",
    "\n",
    "# accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "# precision = precision_score(true_labels, predicted_labels, pos_label='pos')\n",
    "# recall = recall_score(true_labels, predicted_labels, pos_label='pos')\n",
    "# f1 = f1_score(true_labels, predicted_labels, pos_label='pos')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16f28354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6b256_row0_col0, #T_6b256_row1_col1, #T_6b256_row1_col2, #T_6b256_row3_col3, #T_6b256_row4_col3 {\n",
       "  background-color: #0000ff;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6b256_row0_col1, #T_6b256_row0_col2, #T_6b256_row1_col0, #T_6b256_row2_col3 {\n",
       "  background-color: #f0f0f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6b256_row0_col3 {\n",
       "  background-color: #7676f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6b256_row1_col3 {\n",
       "  background-color: #7b7bf9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6b256_row2_col0 {\n",
       "  background-color: #a4a4f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6b256_row2_col1, #T_6b256_row3_col1, #T_6b256_row4_col1 {\n",
       "  background-color: #7c7cf9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6b256_row2_col2 {\n",
       "  background-color: #5858fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6b256_row3_col0, #T_6b256_row4_col0 {\n",
       "  background-color: #7878f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6b256_row3_col2, #T_6b256_row4_col2 {\n",
       "  background-color: #6d6df9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6b256_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >precision</th>\n",
       "      <th class=\"col_heading level0 col1\" >recall</th>\n",
       "      <th class=\"col_heading level0 col2\" >f1-score</th>\n",
       "      <th class=\"col_heading level0 col3\" >support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6b256_level0_row0\" class=\"row_heading level0 row0\" >neg</th>\n",
       "      <td id=\"T_6b256_row0_col0\" class=\"data row0 col0\" >0.860000</td>\n",
       "      <td id=\"T_6b256_row0_col1\" class=\"data row0 col1\" >0.520000</td>\n",
       "      <td id=\"T_6b256_row0_col2\" class=\"data row0 col2\" >0.640000</td>\n",
       "      <td id=\"T_6b256_row0_col3\" class=\"data row0 col3\" >5097.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b256_level0_row1\" class=\"row_heading level0 row1\" >pos</th>\n",
       "      <td id=\"T_6b256_row1_col0\" class=\"data row1 col0\" >0.640000</td>\n",
       "      <td id=\"T_6b256_row1_col1\" class=\"data row1 col1\" >0.910000</td>\n",
       "      <td id=\"T_6b256_row1_col2\" class=\"data row1 col2\" >0.750000</td>\n",
       "      <td id=\"T_6b256_row1_col3\" class=\"data row1 col3\" >4903.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b256_level0_row2\" class=\"row_heading level0 row2\" >accuracy</th>\n",
       "      <td id=\"T_6b256_row2_col0\" class=\"data row2 col0\" >0.710000</td>\n",
       "      <td id=\"T_6b256_row2_col1\" class=\"data row2 col1\" >0.710000</td>\n",
       "      <td id=\"T_6b256_row2_col2\" class=\"data row2 col2\" >0.710000</td>\n",
       "      <td id=\"T_6b256_row2_col3\" class=\"data row2 col3\" >0.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b256_level0_row3\" class=\"row_heading level0 row3\" >macro avg</th>\n",
       "      <td id=\"T_6b256_row3_col0\" class=\"data row3 col0\" >0.750000</td>\n",
       "      <td id=\"T_6b256_row3_col1\" class=\"data row3 col1\" >0.710000</td>\n",
       "      <td id=\"T_6b256_row3_col2\" class=\"data row3 col2\" >0.700000</td>\n",
       "      <td id=\"T_6b256_row3_col3\" class=\"data row3 col3\" >10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b256_level0_row4\" class=\"row_heading level0 row4\" >weighted avg</th>\n",
       "      <td id=\"T_6b256_row4_col0\" class=\"data row4 col0\" >0.750000</td>\n",
       "      <td id=\"T_6b256_row4_col1\" class=\"data row4 col1\" >0.710000</td>\n",
       "      <td id=\"T_6b256_row4_col2\" class=\"data row4 col2\" >0.700000</td>\n",
       "      <td id=\"T_6b256_row4_col3\" class=\"data row4 col3\" >10000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1983dee8190>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = classification_report(true_labels,predicted_labels,output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose().round(2)\n",
    "cm = sns.light_palette(\"blue\", as_cmap=True)\n",
    "df_report.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f796b38",
   "metadata": {},
   "source": [
    "# Analysis of the three models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf9212",
   "metadata": {},
   "source": [
    "Logistic Regression:\n",
    "Evaluation Metrics:\n",
    "Accuracy: 85%\n",
    "Precision: 87%\n",
    "Recall: 85%\n",
    "F1-score: 85%\n",
    "\n",
    "Logistic Regression is a linear classification algorithm that estimates the probability of an input belonging to each class using a logistic function. It assumes a linear relationship between the input features and the log-odds of the output classes.\n",
    "In the context of sentiment analysis, Logistic Regression learns the relationship between the input features (typically a bag-of-words representation of the text) and the sentiment labels (positive or negative). During training, it adjusts the model parameters to maximize the likelihood of the observed sentiment labels given the input features.\n",
    "\n",
    "Linear SVC:\n",
    "Evaluation Metrics:\n",
    "Accuracy: 87%\n",
    "Precision: 89%\n",
    "Recall: 87%\n",
    "F1-score: 88%\n",
    "\n",
    "Linear SVC is a variant of Support Vector Machines (SVM) that uses a linear kernel. SVM is a powerful supervised learning algorithm for classification tasks, including sentiment analysis. The goal of SVM is to find an optimal hyperplane that separates data points of different classes with the largest margin. In the case of sentiment analysis, Linear SVC aims to find a linear decision boundary that separates positive and negative sentiment samples. During training, it identifies a subset of training samples, called support vectors, that are closest to the decision boundary.\n",
    "The model then learns the optimal weights for the linear decision boundary, maximizing the margin between support vectors of different sentiment classes.\n",
    "\n",
    "VADER's ALGORITHM:\n",
    "Evaluation Metrics:\n",
    "Accuracy: 70%\n",
    "Precision: 64%\n",
    "Recall: 91%\n",
    "F1-score: 75%\n",
    "\n",
    "VADER is a rule-based sentiment analysis algorithm specifically designed for sentiment analysis of social media text.\n",
    "It utilizes a lexicon that assigns sentiment scores to words based on their semantic orientation (positive, negative, or neutral). VADER incorporates rules for handling negations, intensifiers, and punctuation to improve sentiment analysis accuracy.\n",
    "The algorithm calculates a sentiment score for a given text based on the presence and intensity of positive and negative words.\n",
    "VADER considers the overall sentiment of the text by combining the individual word scores and applying sentiment intensity modifiers.\n",
    "\n",
    "Therefore , I conclude that implementing the Linear SVC model for sentiment analysis based on the evaluation reports. It outperforms the other two models in terms of accuracy, precision, recall, and F1-score. While effectively addressing high-dimensional feature spaces, linear SVC also provides good performance. In terms of assessment measures, it also outperforms Logistic Regression and VADER. Despite being computationally efficient, VADER does not outperform the other two models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
